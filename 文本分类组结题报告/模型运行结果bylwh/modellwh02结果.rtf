{\rtf1\ansi\ansicpg936\cocoartf2511
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs22 \cf2 \CocoaLigature0 current epoch 1\
1591282359: step 1, loss 1.5552, acc 0.272727\
1591282360: step 2, loss 3.54363, acc 0.272727\
1591282361: step 3, loss 2.28548, acc 0.272727\
1591282362: step 4, loss 1.68761, acc 0.136364\
1591282363: step 5, loss 1.72722, acc 0.227273\
1591282364: step 6, loss 1.57321, acc 0.227273\
1591282364: step 7, loss 1.63794, acc 0.227273\
1591282365: step 8, loss 1.63991, acc 0.272727\
1591282366: step 9, loss 1.59233, acc 0.272727\
1591282367: step 10, loss 1.59073, acc 0.272727\
1591282368: step 11, loss 1.62612, acc 0.227273\
1591282369: step 12, loss 1.80321, acc 0.136364\
1591282370: step 13, loss 1.48706, acc 0.272727\
1591282370: step 14, loss 1.77855, acc 0.227273\
1591282371: step 15, loss 1.50875, acc 0.272727\
1591282372: step 16, loss 1.54469, acc 0.363636\
1591282373: step 17, loss 1.71167, acc 0.363636\
1591282374: step 18, loss 1.42964, acc 0.318182\
1591282375: step 19, loss 1.39469, acc 0.409091\
1591282376: step 20, loss 1.69928, acc 0.227273\
1591282376: step 21, loss 1.6614, acc 0.136364\
1591282377: step 22, loss 1.41061, acc 0.318182\
1591282378: step 23, loss 1.52717, acc 0.272727\
1591282379: step 24, loss 1.60876, acc 0.318182\
1591282380: step 25, loss 1.47213, acc 0.318182\
1591282381: step 26, loss 1.53327, acc 0.318182\
1591282382: step 27, loss 1.54385, acc 0.318182\
1591282383: step 28, loss 1.52413, acc 0.318182\
1591282383: step 29, loss 1.59664, acc 0.181818\
1591282384: step 30, loss 1.51439, acc 0.363636\
1591282385: step 31, loss 1.56044, acc 0.0909091\
1591282386: step 32, loss 1.72209, acc 0.272727\
1591282387: step 33, loss 1.57282, acc 0.409091\
1591282388: step 34, loss 1.53746, acc 0.181818\
1591282389: step 35, loss 1.53328, acc 0.272727\
1591282390: step 36, loss 1.36404, acc 0.454545\
1591282390: step 37, loss 1.40074, acc 0.409091\
1591282391: step 38, loss 1.71345, acc 0.181818\
1591282392: step 39, loss 1.51843, acc 0.272727\
1591282393: step 40, loss 1.36542, acc 0.409091\
1591282394: step 41, loss 1.38046, acc 0.454545\
1591282395: step 42, loss 1.53909, acc 0.227273\
1591282396: step 43, loss 1.56514, acc 0.318182\
1591282396: step 44, loss 1.7637, acc 0.318182\
1591282397: step 45, loss 1.54848, acc 0.272727\
1591282398: step 46, loss 1.71341, acc 0.181818\
1591282399: step 47, loss 1.35289, acc 0.454545\
1591282400: step 48, loss 1.43809, acc 0.318182\
1591282401: step 49, loss 1.56294, acc 0.363636\
1591282402: step 50, loss 1.46522, acc 0.181818\
1591282402: step 51, loss 1.388, acc 0.318182\
1591282403: step 52, loss 1.47392, acc 0.318182\
1591282404: step 53, loss 1.34085, acc 0.409091\
1591282405: step 54, loss 1.70634, acc 0.363636\
1591282406: step 55, loss 1.83268, acc 0.227273\
1591282407: step 56, loss 1.74937, acc 0.227273\
1591282408: step 57, loss 1.6614, acc 0.227273\
1591282408: step 58, loss 1.57697, acc 0.181818\
1591282409: step 59, loss 1.42261, acc 0.363636\
1591282410: step 60, loss 1.3618, acc 0.409091\
1591282411: step 61, loss 1.51885, acc 0.318182\
1591282412: step 62, loss 1.3812, acc 0.363636\
1591282413: step 63, loss 1.48329, acc 0.454545\
1591282414: step 64, loss 1.51223, acc 0.363636\
1591282414: step 65, loss 1.77051, acc 0.318182\
1591282415: step 66, loss 1.51689, acc 0.318182\
1591282416: step 67, loss 1.42184, acc 0.363636\
1591282417: step 68, loss 1.6481, acc 0.363636\
1591282418: step 69, loss 1.41924, acc 0.318182\
1591282419: step 70, loss 1.59042, acc 0.272727\
1591282420: step 71, loss 1.4975, acc 0.227273\
1591282420: step 72, loss 1.36081, acc 0.363636\
1591282421: step 73, loss 1.6529, acc 0.272727\
1591282422: step 74, loss 1.32315, acc 0.409091\
1591282423: step 75, loss 1.45089, acc 0.454545\
1591282424: step 76, loss 1.62751, acc 0.227273\
1591282425: step 77, loss 1.42264, acc 0.363636\
1591282426: step 78, loss 1.47476, acc 0.272727\
1591282427: step 79, loss 1.26193, acc 0.545455\
1591282427: step 80, loss 1.67694, acc 0.272727\
1591282428: step 81, loss 1.6562, acc 0.0909091\
1591282429: step 82, loss 1.39907, acc 0.409091\
1591282430: step 83, loss 1.56244, acc 0.272727\
1591282431: step 84, loss 1.46862, acc 0.272727\
1591282432: step 85, loss 1.37897, acc 0.363636\
1591282433: step 86, loss 1.40898, acc 0.454545\
1591282434: step 87, loss 1.36603, acc 0.5\
1591282434: step 88, loss 1.37188, acc 0.409091\
1591282435: step 89, loss 1.63404, acc 0.181818\
1591282436: step 90, loss 1.36714, acc 0.409091\
1591282437: step 91, loss 1.40435, acc 0.318182\
1591282438: step 92, loss 1.54008, acc 0.363636\
1591282439: step 93, loss 1.77198, acc 0.272727\
1591282440: step 94, loss 1.46466, acc 0.318182\
1591282440: step 95, loss 1.30305, acc 0.363636\
1591282441: step 96, loss 1.55894, acc 0.181818\
1591282442: step 97, loss 1.43657, acc 0.363636\
1591282443: step 98, loss 1.50514, acc 0.454545\
1591282444: step 99, loss 1.40067, acc 0.318182\
1591282445: step 100, loss 1.48663, acc 0.318182\
++++++++++++++++++dev++++++++++++++1591282453: step 100, loss 1.41665, acc 0.376258\
1591282454: step 101, loss 1.78119, acc 0.272727\
1591282455: step 102, loss 1.22601, acc 0.545455\
1591282456: step 103, loss 1.45474, acc 0.272727\
1591282457: step 104, loss 1.32402, acc 0.363636\
1591282457: step 105, loss 1.19105, acc 0.545455\
1591282458: step 106, loss 1.41764, acc 0.272727\
1591282459: step 107, loss 1.24447, acc 0.5\
1591282460: step 108, loss 1.36339, acc 0.363636\
1591282461: step 109, loss 1.50115, acc 0.227273\
1591282462: step 110, loss 1.47232, acc 0.272727\
1591282463: step 111, loss 1.36831, acc 0.318182\
1591282463: step 112, loss 1.35733, acc 0.363636\
1591282464: step 113, loss 1.4449, acc 0.454545\
1591282465: step 114, loss 1.13308, acc 0.454545\
1591282466: step 115, loss 1.04908, acc 0.5\
1591282467: step 116, loss 1.5642, acc 0.227273\
1591282468: step 117, loss 1.27693, acc 0.318182\
1591282468: step 118, loss 1.16668, acc 0.5\
1591282469: step 119, loss 1.45529, acc 0.363636\
1591282470: step 120, loss 1.25082, acc 0.409091\
1591282471: step 121, loss 1.47154, acc 0.363636\
1591282472: step 122, loss 1.26026, acc 0.409091\
1591282473: step 123, loss 1.54175, acc 0.272727\
1591282473: step 124, loss 1.40598, acc 0.363636\
1591282474: step 125, loss 1.2035, acc 0.454545\
1591282475: step 126, loss 1.16939, acc 0.5\
1591282476: step 127, loss 1.29273, acc 0.5\
1591282477: step 128, loss 1.56082, acc 0.5\
1591282478: step 129, loss 1.24034, acc 0.454545\
1591282478: step 130, loss 1.26174, acc 0.545455\
1591282479: step 131, loss 1.23062, acc 0.454545\
1591282480: step 132, loss 1.07865, acc 0.545455\
1591282481: step 133, loss 1.17751, acc 0.590909\
1591282482: step 134, loss 1.06366, acc 0.5\
1591282483: step 135, loss 1.71325, acc 0.454545\
1591282484: step 136, loss 1.46229, acc 0.272727\
1591282484: step 137, loss 1.20391, acc 0.5\
1591282485: step 138, loss 1.38525, acc 0.5\
1591282486: step 139, loss 1.53009, acc 0.272727\
1591282487: step 140, loss 1.32679, acc 0.318182\
1591282488: step 141, loss 1.1106, acc 0.590909\
1591282489: step 142, loss 1.26273, acc 0.454545\
1591282489: step 143, loss 1.04749, acc 0.681818\
1591282490: step 144, loss 1.28873, acc 0.5\
1591282491: step 145, loss 1.25566, acc 0.409091\
1591282492: step 146, loss 1.57608, acc 0.363636\
1591282493: step 147, loss 1.27883, acc 0.363636\
1591282494: step 148, loss 1.21427, acc 0.5\
1591282494: step 149, loss 1.30639, acc 0.318182\
1591282495: step 150, loss 1.24385, acc 0.454545\
1591282496: step 151, loss 1.53015, acc 0.272727\
1591282497: step 152, loss 1.38917, acc 0.409091\
1591282498: step 153, loss 1.19061, acc 0.5\
1591282499: step 154, loss 1.05785, acc 0.636364\
1591282499: step 155, loss 1.38446, acc 0.409091\
1591282500: step 156, loss 1.07611, acc 0.545455\
1591282501: step 157, loss 1.47265, acc 0.363636\
1591282502: step 158, loss 1.27143, acc 0.409091\
1591282503: step 159, loss 1.26608, acc 0.318182\
1591282503: step 160, loss 1.06882, acc 0.5\
1591282504: step 161, loss 1.21874, acc 0.545455\
1591282505: step 162, loss 1.41907, acc 0.454545\
1591282506: step 163, loss 1.28604, acc 0.545455\
1591282507: step 164, loss 1.51747, acc 0.363636\
1591282508: step 165, loss 1.10034, acc 0.5\
1591282509: step 166, loss 1.29795, acc 0.272727\
1591282510: step 167, loss 1.10117, acc 0.590909\
1591282510: step 168, loss 1.17297, acc 0.454545\
1591282511: step 169, loss 1.23083, acc 0.409091\
1591282512: step 170, loss 1.60976, acc 0.181818\
1591282513: step 171, loss 1.24797, acc 0.454545\
1591282514: step 172, loss 1.25158, acc 0.5\
1591282515: step 173, loss 1.27972, acc 0.5\
1591282516: step 174, loss 1.25183, acc 0.363636\
1591282516: step 175, loss 1.30013, acc 0.272727\
1591282517: step 176, loss 1.37378, acc 0.318182\
1591282518: step 177, loss 1.25005, acc 0.363636\
1591282519: step 178, loss 1.12596, acc 0.5\
1591282520: step 179, loss 1.18858, acc 0.409091\
1591282521: step 180, loss 1.45606, acc 0.318182\
1591282521: step 181, loss 1.64014, acc 0.227273\
1591282522: step 182, loss 1.05907, acc 0.545455\
1591282523: step 183, loss 1.15079, acc 0.409091\
1591282524: step 184, loss 1.22288, acc 0.5\
1591282525: step 185, loss 1.27973, acc 0.454545\
1591282526: step 186, loss 1.01058, acc 0.727273\
1591282526: step 187, loss 1.34462, acc 0.363636\
1591282527: step 188, loss 1.24884, acc 0.545455\
1591282528: step 189, loss 1.2595, acc 0.5\
1591282529: step 190, loss 1.20777, acc 0.409091\
1591282530: step 191, loss 1.28923, acc 0.454545\
1591282531: step 192, loss 1.07981, acc 0.545455\
1591282531: step 193, loss 1.29753, acc 0.454545\
1591282532: step 194, loss 1.50216, acc 0.363636\
1591282533: step 195, loss 0.993743, acc 0.590909\
1591282534: step 196, loss 1.05114, acc 0.409091\
1591282535: step 197, loss 1.15653, acc 0.545455\
1591282536: step 198, loss 1.2445, acc 0.5\
1591282536: step 199, loss 1.32011, acc 0.363636\
1591282537: step 200, loss 1.2315, acc 0.5\
++++++++++++++++++dev++++++++++++++1591282544: step 200, loss 1.17855, acc 0.476861\
1591282545: step 201, loss 1.1891, acc 0.636364\
1591282546: step 202, loss 1.28098, acc 0.454545\
1591282546: step 203, loss 0.99099, acc 0.545455\
1591282547: step 204, loss 1.11439, acc 0.357143\
current epoch 2\
1591282548: step 205, loss 1.04094, acc 0.409091\
1591282549: step 206, loss 1.06252, acc 0.363636\
1591282549: step 207, loss 1.07153, acc 0.545455\
1591282550: step 208, loss 0.97532, acc 0.545455\
1591282551: step 209, loss 1.37732, acc 0.454545\
1591282552: step 210, loss 1.04598, acc 0.363636\
1591282553: step 211, loss 1.31996, acc 0.454545\
1591282554: step 212, loss 1.14978, acc 0.363636\
1591282554: step 213, loss 1.10482, acc 0.454545\
1591282555: step 214, loss 1.20431, acc 0.454545\
1591282556: step 215, loss 0.946257, acc 0.590909\
1591282557: step 216, loss 1.2885, acc 0.454545\
1591282558: step 217, loss 1.06516, acc 0.727273\
1591282558: step 218, loss 1.69795, acc 0.363636\
1591282559: step 219, loss 1.14662, acc 0.454545\
1591282560: step 220, loss 1.18874, acc 0.590909\
1591282561: step 221, loss 1.20356, acc 0.454545\
1591282562: step 222, loss 0.818494, acc 0.636364\
1591282563: step 223, loss 1.06046, acc 0.318182\
1591282563: step 224, loss 1.16123, acc 0.545455\
1591282564: step 225, loss 1.08805, acc 0.454545\
1591282565: step 226, loss 1.02148, acc 0.636364\
1591282566: step 227, loss 1.11323, acc 0.590909\
1591282567: step 228, loss 1.00139, acc 0.5\
1591282568: step 229, loss 1.46576, acc 0.272727\
1591282568: step 230, loss 1.10105, acc 0.5\
1591282569: step 231, loss 0.939788, acc 0.454545\
1591282570: step 232, loss 1.31203, acc 0.5\
1591282571: step 233, loss 0.957533, acc 0.5\
1591282572: step 234, loss 1.49727, acc 0.5\
1591282573: step 235, loss 1.20231, acc 0.5\
1591282573: step 236, loss 0.993639, acc 0.590909\
1591282574: step 237, loss 1.17941, acc 0.5\
1591282575: step 238, loss 1.40375, acc 0.454545\
1591282576: step 239, loss 1.31689, acc 0.454545\
1591282577: step 240, loss 0.874171, acc 0.545455\
1591282577: step 241, loss 1.01747, acc 0.5\
1591282578: step 242, loss 1.14408, acc 0.409091\
1591282579: step 243, loss 0.869452, acc 0.590909\
1591282580: step 244, loss 1.04909, acc 0.409091\
1591282581: step 245, loss 1.08482, acc 0.454545\
1591282582: step 246, loss 1.08869, acc 0.5\
1591282582: step 247, loss 1.03992, acc 0.454545\
1591282583: step 248, loss 1.09071, acc 0.545455\
1591282584: step 249, loss 1.37414, acc 0.409091\
1591282585: step 250, loss 1.55482, acc 0.454545\
1591282586: step 251, loss 1.18623, acc 0.545455\
1591282587: step 252, loss 1.30882, acc 0.363636\
1591282587: step 253, loss 0.996955, acc 0.681818\
1591282588: step 254, loss 1.06459, acc 0.454545\
1591282589: step 255, loss 1.00201, acc 0.5\
1591282590: step 256, loss 1.16906, acc 0.409091\
1591282591: step 257, loss 1.01289, acc 0.545455\
1591282591: step 258, loss 1.40995, acc 0.318182\
1591282592: step 259, loss 1.45692, acc 0.318182\
1591282593: step 260, loss 1.29469, acc 0.409091\
1591282594: step 261, loss 1.62668, acc 0.318182\
1591282595: step 262, loss 1.51885, acc 0.181818\
1591282596: step 263, loss 1.20735, acc 0.545455\
1591282596: step 264, loss 1.44441, acc 0.5\
1591282597: step 265, loss 1.18747, acc 0.454545\
1591282598: step 266, loss 0.792489, acc 0.772727\
1591282599: step 267, loss 1.01802, acc 0.545455\
1591282600: step 268, loss 1.01961, acc 0.681818\
1591282601: step 269, loss 1.06027, acc 0.454545\
1591282601: step 270, loss 1.42296, acc 0.545455\
1591282602: step 271, loss 1.00676, acc 0.545455\
1591282603: step 272, loss 1.05348, acc 0.636364\
1591282604: step 273, loss 1.03863, acc 0.590909\
1591282605: step 274, loss 1.19479, acc 0.454545\
1591282605: step 275, loss 1.04417, acc 0.5\
1591282606: step 276, loss 1.24402, acc 0.409091\
1591282607: step 277, loss 1.16752, acc 0.681818\
1591282608: step 278, loss 0.97186, acc 0.545455\
1591282609: step 279, loss 1.11177, acc 0.409091\
1591282610: step 280, loss 1.36382, acc 0.409091\
1591282610: step 281, loss 1.01256, acc 0.409091\
1591282611: step 282, loss 1.18208, acc 0.545455\
1591282612: step 283, loss 1.00217, acc 0.681818\
1591282613: step 284, loss 1.14541, acc 0.5\
1591282614: step 285, loss 1.0667, acc 0.454545\
1591282615: step 286, loss 1.04002, acc 0.590909\
1591282615: step 287, loss 0.961347, acc 0.681818\
1591282616: step 288, loss 0.933795, acc 0.636364\
1591282617: step 289, loss 1.30948, acc 0.5\
1591282618: step 290, loss 1.17547, acc 0.454545\
1591282619: step 291, loss 1.07038, acc 0.590909\
1591282620: step 292, loss 1.16021, acc 0.5\
1591282620: step 293, loss 0.879053, acc 0.545455\
1591282621: step 294, loss 1.10134, acc 0.590909\
1591282622: step 295, loss 0.938915, acc 0.772727\
1591282623: step 296, loss 1.07376, acc 0.5\
1591282624: step 297, loss 1.10608, acc 0.590909\
1591282624: step 298, loss 1.12578, acc 0.5\
1591282625: step 299, loss 0.928102, acc 0.590909\
1591282626: step 300, loss 1.26335, acc 0.454545\
++++++++++++++++++dev++++++++++++++1591282633: step 300, loss 1.12402, acc 0.488934\
1591282634: step 301, loss 1.05989, acc 0.590909\
1591282634: step 302, loss 1.16782, acc 0.454545\
1591282635: step 303, loss 1.01109, acc 0.5\
1591282636: step 304, loss 1.16262, acc 0.5\
1591282637: step 305, loss 1.69009, acc 0.227273\
1591282638: step 306, loss 0.923837, acc 0.545455\
1591282639: step 307, loss 0.967792, acc 0.454545\
1591282639: step 308, loss 1.33046, acc 0.545455\
1591282640: step 309, loss 0.841522, acc 0.636364\
1591282641: step 310, loss 1.3284, acc 0.409091\
1591282642: step 311, loss 1.02461, acc 0.590909\
1591282643: step 312, loss 1.01728, acc 0.5\
1591282643: step 313, loss 1.2299, acc 0.590909\
1591282644: step 314, loss 1.44811, acc 0.454545\
1591282645: step 315, loss 1.0566, acc 0.409091\
1591282646: step 316, loss 0.946304, acc 0.5\
1591282647: step 317, loss 0.927903, acc 0.681818\
1591282648: step 318, loss 0.92286, acc 0.590909\
1591282648: step 319, loss 1.02092, acc 0.5\
1591282649: step 320, loss 1.30797, acc 0.5\
1591282650: step 321, loss 1.18415, acc 0.545455\
1591282651: step 322, loss 1.17012, acc 0.545455\
1591282652: step 323, loss 1.16404, acc 0.454545\
1591282653: step 324, loss 1.20851, acc 0.5\
1591282653: step 325, loss 1.49492, acc 0.363636\
1591282654: step 326, loss 1.20265, acc 0.545455\
1591282655: step 327, loss 0.969797, acc 0.636364\
1591282656: step 328, loss 0.939093, acc 0.636364\
1591282657: step 329, loss 1.12315, acc 0.636364\
1591282657: step 330, loss 1.13146, acc 0.409091\
1591282658: step 331, loss 1.29483, acc 0.409091\
1591282659: step 332, loss 1.16006, acc 0.454545\
1591282660: step 333, loss 1.00994, acc 0.590909\
1591282661: step 334, loss 0.903666, acc 0.590909\
1591282662: step 335, loss 0.870427, acc 0.727273\
1591282662: step 336, loss 0.980008, acc 0.5\
1591282663: step 337, loss 0.700478, acc 0.727273\
1591282664: step 338, loss 1.04686, acc 0.545455\
1591282665: step 339, loss 1.04644, acc 0.636364\
1591282666: step 340, loss 1.1007, acc 0.545455\
1591282667: step 341, loss 1.25292, acc 0.5\
1591282667: step 342, loss 1.13812, acc 0.545455\
1591282668: step 343, loss 1.27415, acc 0.363636\
1591282669: step 344, loss 0.866009, acc 0.727273\
1591282670: step 345, loss 0.89308, acc 0.590909\
1591282671: step 346, loss 0.97127, acc 0.545455\
1591282671: step 347, loss 0.751531, acc 0.636364\
1591282672: step 348, loss 0.686924, acc 0.772727\
1591282673: step 349, loss 0.842552, acc 0.590909\
1591282674: step 350, loss 0.860237, acc 0.636364\
1591282675: step 351, loss 0.969142, acc 0.636364\
1591282676: step 352, loss 0.9836, acc 0.636364\
1591282676: step 353, loss 0.960998, acc 0.454545\
1591282677: step 354, loss 1.11947, acc 0.409091\
1591282678: step 355, loss 1.32276, acc 0.363636\
1591282679: step 356, loss 0.966336, acc 0.545455\
1591282680: step 357, loss 0.960256, acc 0.5\
1591282680: step 358, loss 1.082, acc 0.590909\
1591282681: step 359, loss 1.27407, acc 0.363636\
1591282682: step 360, loss 1.05507, acc 0.590909\
1591282683: step 361, loss 1.04298, acc 0.636364\
1591282684: step 362, loss 1.12175, acc 0.545455\
1591282685: step 363, loss 0.948462, acc 0.454545\
1591282685: step 364, loss 0.861023, acc 0.636364\
1591282686: step 365, loss 1.26881, acc 0.454545\
1591282687: step 366, loss 1.1022, acc 0.636364\
1591282688: step 367, loss 1.31686, acc 0.5\
1591282689: step 368, loss 1.23231, acc 0.454545\
1591282689: step 369, loss 0.992165, acc 0.636364\
1591282690: step 370, loss 0.854183, acc 0.545455\
1591282691: step 371, loss 1.31394, acc 0.409091\
1591282692: step 372, loss 0.76138, acc 0.681818\
1591282693: step 373, loss 1.12335, acc 0.363636\
1591282694: step 374, loss 1.43083, acc 0.318182\
1591282694: step 375, loss 1.09734, acc 0.545455\
1591282695: step 376, loss 1.22498, acc 0.545455\
1591282696: step 377, loss 0.888881, acc 0.454545\
1591282697: step 378, loss 0.959063, acc 0.545455\
1591282698: step 379, loss 1.19065, acc 0.5\
1591282699: step 380, loss 1.08124, acc 0.5\
1591282699: step 381, loss 0.951352, acc 0.545455\
1591282700: step 382, loss 0.999885, acc 0.681818\
1591282701: step 383, loss 1.08918, acc 0.5\
1591282702: step 384, loss 1.20273, acc 0.5\
1591282703: step 385, loss 1.08619, acc 0.681818\
1591282704: step 386, loss 0.769841, acc 0.681818\
1591282704: step 387, loss 1.08141, acc 0.727273\
1591282705: step 388, loss 1.02627, acc 0.636364\
1591282706: step 389, loss 1.04949, acc 0.590909\
1591282707: step 390, loss 0.831754, acc 0.772727\
1591282708: step 391, loss 1.36257, acc 0.363636\
1591282708: step 392, loss 0.837607, acc 0.545455\
1591282709: step 393, loss 0.885281, acc 0.636364\
1591282710: step 394, loss 1.12155, acc 0.454545\
1591282711: step 395, loss 1.19218, acc 0.454545\
1591282712: step 396, loss 0.869778, acc 0.681818\
1591282713: step 397, loss 1.07598, acc 0.5\
1591282713: step 398, loss 1.33664, acc 0.454545\
1591282714: step 399, loss 0.77396, acc 0.681818\
1591282715: step 400, loss 0.968268, acc 0.590909\
++++++++++++++++++dev++++++++++++++1591282722: step 400, loss 1.00032, acc 0.567404\
1591282723: step 401, loss 0.997634, acc 0.636364\
1591282723: step 402, loss 1.22691, acc 0.409091\
1591282724: step 403, loss 1.01192, acc 0.636364\
1591282725: step 404, loss 0.903249, acc 0.590909\
1591282726: step 405, loss 0.953614, acc 0.590909\
1591282727: step 406, loss 1.19618, acc 0.454545\
1591282727: step 407, loss 1.0052, acc 0.545455\
1591282728: step 408, loss 0.924586, acc 0.5\
current epoch 3\
1591282729: step 409, loss 0.725442, acc 0.636364\
1591282730: step 410, loss 0.90344, acc 0.727273\
1591282730: step 411, loss 0.999604, acc 0.545455\
1591282731: step 412, loss 1.13854, acc 0.5\
1591282732: step 413, loss 1.11486, acc 0.454545\
1591282733: step 414, loss 1.10177, acc 0.590909\
1591282734: step 415, loss 1.05754, acc 0.590909\
1591282735: step 416, loss 1.15548, acc 0.454545\
1591282735: step 417, loss 0.930006, acc 0.545455\
1591282736: step 418, loss 1.29392, acc 0.409091\
1591282737: step 419, loss 1.0384, acc 0.590909\
1591282738: step 420, loss 1.29662, acc 0.454545\
1591282739: step 421, loss 0.819568, acc 0.636364\
1591282740: step 422, loss 0.992572, acc 0.636364\
1591282740: step 423, loss 1.15775, acc 0.363636\
1591282741: step 424, loss 1.01271, acc 0.5\
1591282742: step 425, loss 0.901846, acc 0.636364\
1591282743: step 426, loss 1.20228, acc 0.409091\
1591282744: step 427, loss 1.19393, acc 0.590909\
1591282744: step 428, loss 1.38576, acc 0.363636\
1591282745: step 429, loss 1.16113, acc 0.545455\
1591282746: step 430, loss 0.999348, acc 0.545455\
1591282747: step 431, loss 1.0473, acc 0.454545\
1591282748: step 432, loss 0.816374, acc 0.636364\
1591282749: step 433, loss 0.864062, acc 0.636364\
1591282749: step 434, loss 0.957288, acc 0.636364\
1591282750: step 435, loss 0.817322, acc 0.636364\
1591282751: step 436, loss 1.29676, acc 0.363636\
1591282752: step 437, loss 0.664181, acc 0.818182\
1591282753: step 438, loss 1.16076, acc 0.636364\
1591282753: step 439, loss 1.01707, acc 0.545455\
1591282754: step 440, loss 0.691277, acc 0.681818\
1591282755: step 441, loss 1.10187, acc 0.681818\
1591282756: step 442, loss 1.07534, acc 0.636364\
1591282757: step 443, loss 1.30286, acc 0.545455\
1591282758: step 444, loss 0.970551, acc 0.590909\
1591282758: step 445, loss 0.932321, acc 0.681818\
1591282759: step 446, loss 0.922601, acc 0.818182\
1591282760: step 447, loss 0.801689, acc 0.681818\
1591282761: step 448, loss 1.10895, acc 0.636364\
1591282762: step 449, loss 0.900401, acc 0.636364\
1591282762: step 450, loss 0.702497, acc 0.681818\
1591282763: step 451, loss 0.589405, acc 0.818182\
1591282764: step 452, loss 0.977253, acc 0.681818\
1591282765: step 453, loss 1.24077, acc 0.454545\
1591282766: step 454, loss 1.26579, acc 0.409091\
1591282767: step 455, loss 0.973657, acc 0.590909\
1591282767: step 456, loss 1.06877, acc 0.454545\
1591282768: step 457, loss 0.929423, acc 0.590909\
1591282769: step 458, loss 1.04435, acc 0.681818\
1591282770: step 459, loss 0.820641, acc 0.590909\
1591282771: step 460, loss 1.23006, acc 0.454545\
1591282772: step 461, loss 0.825738, acc 0.727273\
1591282772: step 462, loss 1.051, acc 0.636364\
1591282773: step 463, loss 1.02503, acc 0.5\
1591282774: step 464, loss 0.74759, acc 0.727273\
1591282775: step 465, loss 1.15638, acc 0.681818\
1591282776: step 466, loss 1.02397, acc 0.5\
1591282776: step 467, loss 0.893127, acc 0.636364\
1591282777: step 468, loss 1.13261, acc 0.681818\
1591282778: step 469, loss 0.985355, acc 0.636364\
1591282779: step 470, loss 0.700933, acc 0.727273\
1591282780: step 471, loss 0.804627, acc 0.636364\
1591282781: step 472, loss 1.02489, acc 0.681818\
1591282781: step 473, loss 0.650402, acc 0.772727\
1591282782: step 474, loss 1.1672, acc 0.590909\
1591282783: step 475, loss 0.739842, acc 0.681818\
1591282784: step 476, loss 0.877793, acc 0.590909\
1591282785: step 477, loss 0.766661, acc 0.772727\
1591282786: step 478, loss 1.0141, acc 0.636364\
1591282786: step 479, loss 1.07484, acc 0.590909\
1591282787: step 480, loss 1.01235, acc 0.590909\
1591282788: step 481, loss 0.972923, acc 0.636364\
1591282789: step 482, loss 0.843333, acc 0.5\
1591282790: step 483, loss 1.00667, acc 0.590909\
1591282790: step 484, loss 1.12749, acc 0.5\
1591282791: step 485, loss 1.04455, acc 0.5\
1591282792: step 486, loss 0.91797, acc 0.545455\
1591282793: step 487, loss 0.917653, acc 0.681818\
1591282794: step 488, loss 1.02029, acc 0.545455\
1591282795: step 489, loss 0.90607, acc 0.545455\
1591282795: step 490, loss 0.701118, acc 0.681818\
1591282796: step 491, loss 0.650987, acc 0.818182\
1591282797: step 492, loss 0.86932, acc 0.590909\
1591282798: step 493, loss 0.973068, acc 0.545455\
1591282799: step 494, loss 0.844593, acc 0.636364\
1591282799: step 495, loss 0.605181, acc 0.818182\
1591282800: step 496, loss 0.912782, acc 0.636364\
1591282801: step 497, loss 0.855378, acc 0.636364\
1591282802: step 498, loss 0.869516, acc 0.636364\
1591282803: step 499, loss 0.540599, acc 0.818182\
1591282804: step 500, loss 0.710435, acc 0.681818\
++++++++++++++++++dev++++++++++++++1591282810: step 500, loss 0.997541, acc 0.587525\
1591282811: step 501, loss 1.00057, acc 0.727273\
1591282812: step 502, loss 0.906579, acc 0.727273\
1591282813: step 503, loss 0.792649, acc 0.636364\
1591282813: step 504, loss 1.22014, acc 0.636364\
1591282814: step 505, loss 1.00722, acc 0.590909\
1591282815: step 506, loss 1.24739, acc 0.5\
1591282816: step 507, loss 0.583767, acc 0.727273\
1591282817: step 508, loss 0.930329, acc 0.636364\
1591282818: step 509, loss 1.0903, acc 0.590909\
1591282818: step 510, loss 0.912099, acc 0.545455\
1591282819: step 511, loss 0.73462, acc 0.590909\
1591282820: step 512, loss 1.10688, acc 0.727273\
1591282821: step 513, loss 0.694373, acc 0.636364\
1591282822: step 514, loss 0.965268, acc 0.545455\
1591282823: step 515, loss 0.859368, acc 0.681818\
1591282823: step 516, loss 0.852116, acc 0.636364\
1591282824: step 517, loss 0.730315, acc 0.636364\
1591282825: step 518, loss 0.911933, acc 0.5\
1591282826: step 519, loss 0.691146, acc 0.681818\
1591282827: step 520, loss 0.875595, acc 0.636364\
1591282827: step 521, loss 0.915875, acc 0.636364\
1591282828: step 522, loss 1.04121, acc 0.636364\
1591282829: step 523, loss 0.901144, acc 0.772727\
1591282830: step 524, loss 0.872782, acc 0.590909\
1591282831: step 525, loss 0.99537, acc 0.590909\
1591282832: step 526, loss 0.89185, acc 0.545455\
1591282832: step 527, loss 0.90957, acc 0.681818\
1591282833: step 528, loss 0.83135, acc 0.545455\
1591282834: step 529, loss 1.38774, acc 0.454545\
1591282835: step 530, loss 0.795315, acc 0.636364\
1591282836: step 531, loss 0.75273, acc 0.636364\
1591282837: step 532, loss 0.744604, acc 0.727273\
1591282837: step 533, loss 0.839053, acc 0.727273\
1591282838: step 534, loss 1.03292, acc 0.545455\
1591282839: step 535, loss 1.08322, acc 0.5\
1591282840: step 536, loss 0.643216, acc 0.681818\
1591282841: step 537, loss 1.07608, acc 0.590909\
1591282841: step 538, loss 0.882836, acc 0.590909\
1591282842: step 539, loss 0.646589, acc 0.818182\
1591282843: step 540, loss 1.06217, acc 0.590909\
1591282844: step 541, loss 0.576816, acc 0.727273\
1591282845: step 542, loss 0.881984, acc 0.636364\
1591282846: step 543, loss 0.705769, acc 0.772727\
1591282846: step 544, loss 1.1004, acc 0.409091\
1591282847: step 545, loss 0.700772, acc 0.772727\
1591282848: step 546, loss 0.929123, acc 0.590909\
1591282849: step 547, loss 0.78131, acc 0.636364\
1591282850: step 548, loss 0.686645, acc 0.727273\
1591282851: step 549, loss 1.04179, acc 0.590909\
1591282851: step 550, loss 1.10771, acc 0.545455\
1591282852: step 551, loss 0.618432, acc 0.681818\
1591282853: step 552, loss 0.658265, acc 0.727273\
1591282854: step 553, loss 0.830021, acc 0.681818\
1591282855: step 554, loss 0.748993, acc 0.818182\
1591282855: step 555, loss 0.96712, acc 0.590909\
1591282856: step 556, loss 0.752117, acc 0.681818\
1591282857: step 557, loss 0.927934, acc 0.590909\
1591282858: step 558, loss 0.896488, acc 0.681818\
1591282859: step 559, loss 1.20385, acc 0.590909\
1591282859: step 560, loss 0.956789, acc 0.454545\
1591282860: step 561, loss 0.731715, acc 0.681818\
1591282861: step 562, loss 0.929553, acc 0.636364\
1591282862: step 563, loss 1.23101, acc 0.5\
1591282863: step 564, loss 0.827299, acc 0.681818\
1591282864: step 565, loss 0.933078, acc 0.5\
1591282864: step 566, loss 0.924557, acc 0.5\
1591282865: step 567, loss 0.838435, acc 0.681818\
1591282866: step 568, loss 1.20284, acc 0.409091\
1591282867: step 569, loss 0.822302, acc 0.681818\
1591282868: step 570, loss 1.2376, acc 0.636364\
1591282869: step 571, loss 0.736908, acc 0.727273\
1591282869: step 572, loss 0.716189, acc 0.727273\
1591282870: step 573, loss 1.01022, acc 0.636364\
1591282871: step 574, loss 0.43924, acc 0.772727\
1591282872: step 575, loss 0.689275, acc 0.681818\
1591282873: step 576, loss 0.56854, acc 0.818182\
1591282873: step 577, loss 1.01901, acc 0.545455\
1591282874: step 578, loss 1.17513, acc 0.5\
1591282875: step 579, loss 0.62603, acc 0.727273\
1591282876: step 580, loss 0.836246, acc 0.636364\
1591282877: step 581, loss 0.531427, acc 0.727273\
1591282878: step 582, loss 0.649092, acc 0.727273\
1591282878: step 583, loss 0.988609, acc 0.590909\
1591282879: step 584, loss 0.982973, acc 0.590909\
1591282880: step 585, loss 0.754649, acc 0.727273\
1591282881: step 586, loss 1.05772, acc 0.545455\
1591282882: step 587, loss 1.08007, acc 0.590909\
1591282882: step 588, loss 0.850373, acc 0.681818\
1591282883: step 589, loss 0.677503, acc 0.727273\
1591282884: step 590, loss 0.930475, acc 0.681818\
1591282885: step 591, loss 0.725139, acc 0.727273\
1591282886: step 592, loss 0.908975, acc 0.5\
1591282887: step 593, loss 1.12569, acc 0.590909\
1591282887: step 594, loss 0.541208, acc 0.818182\
1591282888: step 595, loss 0.759823, acc 0.727273\
1591282889: step 596, loss 0.867584, acc 0.636364\
1591282890: step 597, loss 0.63407, acc 0.772727\
1591282891: step 598, loss 0.897557, acc 0.590909\
1591282891: step 599, loss 1.18988, acc 0.5\
1591282892: step 600, loss 0.852991, acc 0.727273\
++++++++++++++++++dev++++++++++++++1591282899: step 600, loss 0.918683, acc 0.597585\
1591282900: step 601, loss 0.978937, acc 0.5\
1591282901: step 602, loss 1.46629, acc 0.5\
1591282901: step 603, loss 1.02206, acc 0.590909\
1591282902: step 604, loss 0.920367, acc 0.5\
1591282903: step 605, loss 1.05187, acc 0.545455\
1591282904: step 606, loss 1.01879, acc 0.5\
1591282905: step 607, loss 0.975312, acc 0.590909\
1591282906: step 608, loss 0.867699, acc 0.590909\
1591282906: step 609, loss 0.840001, acc 0.636364\
1591282907: step 610, loss 1.26281, acc 0.409091\
1591282908: step 611, loss 0.692335, acc 0.772727\
1591282909: step 612, loss 0.673742, acc 0.785714\
current epoch 4\
1591282909: step 613, loss 0.542514, acc 0.772727\
1591282910: step 614, loss 0.656779, acc 0.727273\
1591282911: step 615, loss 1.03059, acc 0.636364\
1591282912: step 616, loss 0.843673, acc 0.636364\
1591282913: step 617, loss 0.89535, acc 0.636364\
1591282914: step 618, loss 0.700987, acc 0.863636\
1591282914: step 619, loss 0.813112, acc 0.681818\
1591282915: step 620, loss 0.747338, acc 0.727273\
1591282916: step 621, loss 0.560329, acc 0.727273\
1591282917: step 622, loss 1.06319, acc 0.545455\
1591282918: step 623, loss 0.91495, acc 0.681818\
1591282919: step 624, loss 1.01187, acc 0.590909\
1591282919: step 625, loss 0.589257, acc 0.727273\
1591282920: step 626, loss 0.714651, acc 0.590909\
1591282921: step 627, loss 0.772575, acc 0.636364\
1591282922: step 628, loss 0.481164, acc 0.863636\
1591282923: step 629, loss 0.784048, acc 0.590909\
1591282924: step 630, loss 0.724813, acc 0.681818\
1591282924: step 631, loss 0.936694, acc 0.545455\
1591282925: step 632, loss 0.580796, acc 0.727273\
1591282926: step 633, loss 1.04222, acc 0.590909\
1591282927: step 634, loss 0.670074, acc 0.727273\
1591282928: step 635, loss 0.595102, acc 0.772727\
1591282928: step 636, loss 0.364315, acc 0.863636\
1591282929: step 637, loss 0.580059, acc 0.727273\
1591282930: step 638, loss 0.750281, acc 0.772727\
1591282931: step 639, loss 0.487441, acc 0.863636\
1591282932: step 640, loss 0.881139, acc 0.727273\
1591282933: step 641, loss 0.782432, acc 0.636364\
1591282933: step 642, loss 0.806151, acc 0.681818\
1591282934: step 643, loss 0.808126, acc 0.590909\
1591282935: step 644, loss 1.07108, acc 0.545455\
1591282936: step 645, loss 0.807566, acc 0.545455\
1591282937: step 646, loss 0.784915, acc 0.727273\
1591282938: step 647, loss 0.899391, acc 0.590909\
1591282938: step 648, loss 0.979217, acc 0.5\
1591282939: step 649, loss 0.419655, acc 0.818182\
1591282940: step 650, loss 0.803263, acc 0.636364\
1591282941: step 651, loss 0.620026, acc 0.772727\
1591282942: step 652, loss 0.903286, acc 0.590909\
1591282942: step 653, loss 0.969471, acc 0.681818\
1591282943: step 654, loss 0.549424, acc 0.772727\
1591282944: step 655, loss 0.715355, acc 0.681818\
1591282945: step 656, loss 0.90047, acc 0.636364\
1591282946: step 657, loss 0.746156, acc 0.863636\
1591282947: step 658, loss 1.06503, acc 0.545455\
1591282947: step 659, loss 0.817686, acc 0.590909\
1591282948: step 660, loss 0.766605, acc 0.636364\
1591282949: step 661, loss 1.02558, acc 0.590909\
1591282950: step 662, loss 0.82454, acc 0.636364\
1591282951: step 663, loss 0.651631, acc 0.772727\
1591282952: step 664, loss 0.796781, acc 0.636364\
1591282952: step 665, loss 0.733879, acc 0.681818\
1591282953: step 666, loss 0.842889, acc 0.681818\
1591282954: step 667, loss 0.815659, acc 0.590909\
1591282955: step 668, loss 0.662083, acc 0.681818\
1591282956: step 669, loss 0.995096, acc 0.636364\
1591282956: step 670, loss 0.876477, acc 0.636364\
1591282957: step 671, loss 0.868303, acc 0.590909\
1591282958: step 672, loss 1.08879, acc 0.636364\
1591282959: step 673, loss 0.886363, acc 0.681818\
1591282960: step 674, loss 0.535729, acc 0.727273\
1591282961: step 675, loss 0.639674, acc 0.727273\
1591282961: step 676, loss 0.815868, acc 0.681818\
1591282962: step 677, loss 0.846096, acc 0.681818\
1591282963: step 678, loss 0.703963, acc 0.636364\
1591282964: step 679, loss 0.603272, acc 0.727273\
1591282965: step 680, loss 0.850203, acc 0.590909\
1591282966: step 681, loss 0.67347, acc 0.772727\
1591282966: step 682, loss 0.851597, acc 0.681818\
1591282967: step 683, loss 0.698663, acc 0.818182\
1591282968: step 684, loss 0.774322, acc 0.681818\
1591282969: step 685, loss 0.854229, acc 0.772727\
1591282970: step 686, loss 0.60386, acc 0.818182\
1591282970: step 687, loss 0.725394, acc 0.727273\
1591282971: step 688, loss 1.26203, acc 0.636364\
1591282972: step 689, loss 0.659979, acc 0.772727\
1591282973: step 690, loss 0.835391, acc 0.636364\
1591282974: step 691, loss 0.736924, acc 0.681818\
1591282975: step 692, loss 0.780946, acc 0.636364\
1591282975: step 693, loss 0.645787, acc 0.772727\
1591282976: step 694, loss 0.537795, acc 0.818182\
1591282977: step 695, loss 0.383326, acc 0.818182\
1591282978: step 696, loss 0.60842, acc 0.681818\
1591282979: step 697, loss 0.72547, acc 0.681818\
1591282980: step 698, loss 0.824442, acc 0.727273\
1591282980: step 699, loss 0.449377, acc 0.818182\
1591282981: step 700, loss 0.876861, acc 0.636364\
++++++++++++++++++dev++++++++++++++1591282988: step 700, loss 0.855139, acc 0.672032\
1591282989: step 701, loss 0.681097, acc 0.681818\
1591282990: step 702, loss 0.55713, acc 0.818182\
1591282990: step 703, loss 0.424361, acc 0.818182\
1591282991: step 704, loss 0.493561, acc 0.818182\
1591282992: step 705, loss 1.01576, acc 0.590909\
1591282993: step 706, loss 0.895649, acc 0.636364\
1591282994: step 707, loss 0.675587, acc 0.772727\
1591282994: step 708, loss 1.17361, acc 0.636364\
1591282995: step 709, loss 1.06569, acc 0.5\
1591282996: step 710, loss 0.865739, acc 0.727273\
1591282997: step 711, loss 0.391113, acc 0.772727\
1591282998: step 712, loss 0.985537, acc 0.590909\
1591282999: step 713, loss 0.845765, acc 0.636364\
1591282999: step 714, loss 0.477597, acc 0.772727\
1591283000: step 715, loss 0.764637, acc 0.636364\
1591283001: step 716, loss 0.809597, acc 0.818182\
1591283002: step 717, loss 0.558649, acc 0.727273\
1591283003: step 718, loss 0.788791, acc 0.727273\
1591283004: step 719, loss 0.831631, acc 0.636364\
1591283004: step 720, loss 0.918464, acc 0.590909\
1591283005: step 721, loss 0.487559, acc 0.818182\
1591283006: step 722, loss 0.843366, acc 0.636364\
1591283007: step 723, loss 0.803897, acc 0.681818\
1591283008: step 724, loss 0.694963, acc 0.727273\
1591283009: step 725, loss 0.785521, acc 0.636364\
1591283009: step 726, loss 0.477969, acc 0.818182\
1591283010: step 727, loss 0.862944, acc 0.590909\
1591283011: step 728, loss 0.554983, acc 0.818182\
1591283012: step 729, loss 0.743076, acc 0.681818\
1591283013: step 730, loss 0.883456, acc 0.681818\
1591283014: step 731, loss 0.822905, acc 0.636364\
1591283014: step 732, loss 0.753947, acc 0.636364\
1591283015: step 733, loss 1.07281, acc 0.5\
1591283016: step 734, loss 0.819225, acc 0.545455\
1591283017: step 735, loss 0.711392, acc 0.681818\
1591283018: step 736, loss 0.822231, acc 0.590909\
1591283018: step 737, loss 0.909904, acc 0.681818\
1591283019: step 738, loss 0.854624, acc 0.590909\
1591283020: step 739, loss 0.44091, acc 0.954545\
1591283021: step 740, loss 0.5166, acc 0.681818\
1591283022: step 741, loss 0.805177, acc 0.590909\
1591283023: step 742, loss 0.704137, acc 0.681818\
1591283023: step 743, loss 0.478968, acc 0.772727\
1591283024: step 744, loss 0.468378, acc 0.818182\
1591283025: step 745, loss 0.630119, acc 0.727273\
1591283026: step 746, loss 0.706751, acc 0.727273\
1591283027: step 747, loss 1.2581, acc 0.545455\
1591283028: step 748, loss 0.709072, acc 0.772727\
1591283028: step 749, loss 0.637798, acc 0.727273\
1591283029: step 750, loss 0.551402, acc 0.727273\
1591283030: step 751, loss 1.07753, acc 0.590909\
1591283031: step 752, loss 0.756997, acc 0.818182\
1591283032: step 753, loss 0.684668, acc 0.772727\
1591283032: step 754, loss 0.597526, acc 0.772727\
1591283033: step 755, loss 0.525704, acc 0.772727\
1591283034: step 756, loss 0.895843, acc 0.636364\
1591283035: step 757, loss 0.733649, acc 0.681818\
1591283036: step 758, loss 0.716453, acc 0.636364\
1591283037: step 759, loss 0.62621, acc 0.772727\
1591283037: step 760, loss 0.728949, acc 0.681818\
1591283038: step 761, loss 0.709911, acc 0.636364\
1591283039: step 762, loss 0.661349, acc 0.727273\
1591283040: step 763, loss 0.797714, acc 0.590909\
1591283041: step 764, loss 0.674692, acc 0.818182\
1591283042: step 765, loss 0.700379, acc 0.727273\
1591283042: step 766, loss 0.860571, acc 0.590909\
1591283043: step 767, loss 0.93091, acc 0.636364\
1591283044: step 768, loss 0.68187, acc 0.636364\
1591283045: step 769, loss 0.877149, acc 0.636364\
1591283046: step 770, loss 0.475509, acc 0.863636\
1591283046: step 771, loss 0.884577, acc 0.545455\
1591283047: step 772, loss 0.369885, acc 0.909091\
1591283048: step 773, loss 1.15481, acc 0.590909\
1591283049: step 774, loss 0.828943, acc 0.727273\
1591283050: step 775, loss 0.613663, acc 0.818182\
1591283051: step 776, loss 0.894366, acc 0.636364\
1591283051: step 777, loss 0.7753, acc 0.727273\
1591283052: step 778, loss 0.831214, acc 0.727273\
1591283053: step 779, loss 0.524596, acc 0.818182\
1591283054: step 780, loss 0.399379, acc 0.863636\
1591283055: step 781, loss 0.77765, acc 0.590909\
1591283056: step 782, loss 0.90067, acc 0.545455\
1591283056: step 783, loss 0.491315, acc 0.818182\
1591283057: step 784, loss 0.860458, acc 0.727273\
1591283058: step 785, loss 0.79813, acc 0.681818\
1591283059: step 786, loss 0.473848, acc 0.727273\
1591283060: step 787, loss 0.973605, acc 0.772727\
1591283060: step 788, loss 0.964207, acc 0.545455\
1591283061: step 789, loss 0.590307, acc 0.772727\
1591283062: step 790, loss 1.08301, acc 0.681818\
1591283063: step 791, loss 0.904671, acc 0.818182\
1591283064: step 792, loss 0.718373, acc 0.727273\
1591283065: step 793, loss 0.916699, acc 0.590909\
1591283065: step 794, loss 0.795612, acc 0.727273\
1591283066: step 795, loss 0.554194, acc 0.727273\
1591283067: step 796, loss 0.745863, acc 0.636364\
1591283068: step 797, loss 1.06465, acc 0.545455\
1591283069: step 798, loss 0.598077, acc 0.727273\
1591283070: step 799, loss 0.835363, acc 0.590909\
1591283070: step 800, loss 0.801137, acc 0.636364\
++++++++++++++++++dev++++++++++++++1591283077: step 800, loss 0.918158, acc 0.65996\
1591283078: step 801, loss 0.474237, acc 0.818182\
1591283079: step 802, loss 0.759525, acc 0.727273\
1591283080: step 803, loss 0.983585, acc 0.5\
1591283080: step 804, loss 0.766164, acc 0.727273\
1591283081: step 805, loss 0.675183, acc 0.772727\
1591283082: step 806, loss 0.965515, acc 0.636364\
1591283083: step 807, loss 0.310626, acc 0.909091\
1591283084: step 808, loss 0.622746, acc 0.727273\
1591283085: step 809, loss 0.922136, acc 0.590909\
1591283085: step 810, loss 0.632044, acc 0.772727\
1591283086: step 811, loss 0.718241, acc 0.636364\
1591283087: step 812, loss 0.807808, acc 0.590909\
1591283088: step 813, loss 0.658199, acc 0.818182\
1591283089: step 814, loss 0.914369, acc 0.590909\
1591283089: step 815, loss 0.535421, acc 0.727273\
1591283090: step 816, loss 0.367442, acc 0.928571\
current epoch 5\
1591283091: step 817, loss 0.329641, acc 0.909091\
1591283092: step 818, loss 0.553148, acc 0.727273\
1591283092: step 819, loss 0.566535, acc 0.727273\
1591283093: step 820, loss 0.877091, acc 0.636364\
1591283094: step 821, loss 0.55938, acc 0.727273\
1591283095: step 822, loss 0.438192, acc 0.863636\
1591283096: step 823, loss 1.01037, acc 0.5\
1591283097: step 824, loss 0.765001, acc 0.772727\
1591283097: step 825, loss 0.821976, acc 0.681818\
1591283098: step 826, loss 1.3168, acc 0.545455\
1591283099: step 827, loss 0.668453, acc 0.772727\
1591283100: step 828, loss 0.904681, acc 0.681818\
1591283101: step 829, loss 0.895941, acc 0.681818\
1591283102: step 830, loss 0.676519, acc 0.681818\
1591283102: step 831, loss 0.705656, acc 0.545455\
1591283103: step 832, loss 0.674085, acc 0.727273\
1591283104: step 833, loss 0.745425, acc 0.636364\
1591283105: step 834, loss 0.639887, acc 0.772727\
1591283106: step 835, loss 0.806545, acc 0.5\
1591283107: step 836, loss 0.663394, acc 0.681818\
1591283107: step 837, loss 0.685975, acc 0.681818\
1591283108: step 838, loss 0.61942, acc 0.681818\
1591283109: step 839, loss 0.411729, acc 0.909091\
1591283110: step 840, loss 0.382363, acc 0.818182\
1591283111: step 841, loss 0.501442, acc 0.863636\
1591283111: step 842, loss 0.3493, acc 0.954545\
1591283112: step 843, loss 0.553005, acc 0.772727\
1591283113: step 844, loss 0.887635, acc 0.590909\
1591283114: step 845, loss 0.687568, acc 0.772727\
1591283115: step 846, loss 0.820487, acc 0.636364\
1591283116: step 847, loss 0.905688, acc 0.636364\
1591283116: step 848, loss 0.522761, acc 0.863636\
1591283117: step 849, loss 0.667331, acc 0.727273\
1591283118: step 850, loss 0.725539, acc 0.772727\
1591283119: step 851, loss 0.763154, acc 0.727273\
1591283120: step 852, loss 0.844652, acc 0.590909\
1591283121: step 853, loss 0.596874, acc 0.636364\
1591283121: step 854, loss 0.631092, acc 0.727273\
1591283122: step 855, loss 0.562397, acc 0.727273\
1591283123: step 856, loss 1.03316, acc 0.636364\
1591283124: step 857, loss 0.414732, acc 0.863636\
1591283125: step 858, loss 0.921599, acc 0.681818\
1591283125: step 859, loss 0.523287, acc 0.681818\
1591283126: step 860, loss 0.8359, acc 0.636364\
1591283127: step 861, loss 1.0602, acc 0.5\
1591283128: step 862, loss 0.709414, acc 0.727273\
1591283129: step 863, loss 0.683945, acc 0.727273\
1591283130: step 864, loss 0.846361, acc 0.727273\
1591283130: step 865, loss 0.818842, acc 0.681818\
1591283131: step 866, loss 0.422843, acc 0.909091\
1591283132: step 867, loss 0.446805, acc 0.863636\
1591283133: step 868, loss 0.968363, acc 0.409091\
1591283134: step 869, loss 0.543256, acc 0.772727\
1591283135: step 870, loss 0.91299, acc 0.636364\
1591283135: step 871, loss 0.502807, acc 0.818182\
1591283136: step 872, loss 0.765081, acc 0.772727\
1591283137: step 873, loss 0.711693, acc 0.772727\
1591283138: step 874, loss 0.886032, acc 0.636364\
1591283139: step 875, loss 0.995341, acc 0.5\
1591283140: step 876, loss 1.07672, acc 0.681818\
1591283140: step 877, loss 0.989465, acc 0.590909\
1591283141: step 878, loss 0.835202, acc 0.636364\
1591283142: step 879, loss 0.712058, acc 0.681818\
1591283143: step 880, loss 0.798714, acc 0.636364\
1591283144: step 881, loss 0.511113, acc 0.818182\
1591283144: step 882, loss 0.674, acc 0.727273\
1591283145: step 883, loss 0.488905, acc 0.772727\
1591283146: step 884, loss 0.613488, acc 0.727273\
1591283147: step 885, loss 0.652725, acc 0.772727\
1591283148: step 886, loss 0.914541, acc 0.636364\
1591283149: step 887, loss 0.674072, acc 0.818182\
1591283149: step 888, loss 1.22209, acc 0.590909\
1591283150: step 889, loss 0.649774, acc 0.772727\
1591283151: step 890, loss 0.614656, acc 0.636364\
1591283152: step 891, loss 0.858528, acc 0.590909\
1591283153: step 892, loss 1.34397, acc 0.545455\
1591283154: step 893, loss 0.692378, acc 0.681818\
1591283154: step 894, loss 0.995435, acc 0.545455\
1591283155: step 895, loss 0.533292, acc 0.772727\
1591283156: step 896, loss 0.625654, acc 0.590909\
1591283157: step 897, loss 0.677445, acc 0.636364\
1591283158: step 898, loss 0.453566, acc 0.818182\
1591283159: step 899, loss 0.487689, acc 0.818182\
1591283159: step 900, loss 0.584772, acc 0.772727\
++++++++++++++++++dev++++++++++++++1591283166: step 900, loss 0.886657, acc 0.65996\
1591283167: step 901, loss 0.821113, acc 0.727273\
1591283168: step 902, loss 0.642754, acc 0.681818\
1591283169: step 903, loss 0.411172, acc 0.772727\
1591283169: step 904, loss 0.869986, acc 0.681818\
1591283170: step 905, loss 0.629952, acc 0.681818\
1591283171: step 906, loss 0.74321, acc 0.636364\
1591283172: step 907, loss 0.662251, acc 0.727273\
1591283173: step 908, loss 0.697457, acc 0.727273\
1591283174: step 909, loss 0.925392, acc 0.5\
1591283174: step 910, loss 0.874849, acc 0.681818\
1591283175: step 911, loss 0.880609, acc 0.727273\
1591283176: step 912, loss 1.08844, acc 0.5\
1591283177: step 913, loss 0.942073, acc 0.590909\
1591283178: step 914, loss 0.966542, acc 0.772727\
1591283178: step 915, loss 0.373101, acc 0.863636\
1591283179: step 916, loss 1.14387, acc 0.5\
1591283180: step 917, loss 0.67981, acc 0.727273\
1591283181: step 918, loss 0.736104, acc 0.818182\
1591283182: step 919, loss 0.626915, acc 0.772727\
1591283183: step 920, loss 0.906524, acc 0.772727\
1591283183: step 921, loss 0.698119, acc 0.545455\
1591283184: step 922, loss 0.75457, acc 0.636364\
1591283185: step 923, loss 0.865518, acc 0.681818\
1591283186: step 924, loss 0.694343, acc 0.681818\
1591283187: step 925, loss 0.88667, acc 0.636364\
1591283188: step 926, loss 1.07265, acc 0.545455\
1591283188: step 927, loss 0.802415, acc 0.681818\
1591283189: step 928, loss 0.808862, acc 0.681818\
1591283190: step 929, loss 0.885664, acc 0.727273\
1591283191: step 930, loss 0.428955, acc 0.909091\
1591283192: step 931, loss 0.829785, acc 0.636364\
1591283193: step 932, loss 0.958708, acc 0.590909\
1591283193: step 933, loss 0.771489, acc 0.590909\
1591283194: step 934, loss 1.0806, acc 0.636364\
1591283195: step 935, loss 0.891499, acc 0.681818\
1591283196: step 936, loss 0.849265, acc 0.636364\
1591283197: step 937, loss 0.846897, acc 0.5\
1591283197: step 938, loss 0.689079, acc 0.772727\
1591283198: step 939, loss 0.678638, acc 0.681818\
1591283199: step 940, loss 0.785704, acc 0.636364\
1591283200: step 941, loss 0.601049, acc 0.772727\
1591283201: step 942, loss 0.753958, acc 0.818182\
1591283202: step 943, loss 0.775073, acc 0.636364\
1591283202: step 944, loss 0.516162, acc 0.727273\
1591283203: step 945, loss 1.40534, acc 0.681818\
1591283204: step 946, loss 0.539527, acc 0.681818\
1591283205: step 947, loss 0.671791, acc 0.727273\
1591283206: step 948, loss 0.543926, acc 0.818182\
1591283207: step 949, loss 0.523497, acc 0.818182\
1591283207: step 950, loss 0.555365, acc 0.818182\
1591283208: step 951, loss 0.635578, acc 0.818182\
1591283209: step 952, loss 0.818358, acc 0.727273\
1591283210: step 953, loss 0.699014, acc 0.772727\
1591283211: step 954, loss 0.865128, acc 0.636364\
1591283211: step 955, loss 0.617025, acc 0.818182\
1591283212: step 956, loss 0.690397, acc 0.772727\
1591283213: step 957, loss 0.703593, acc 0.681818\
1591283214: step 958, loss 0.74672, acc 0.681818\
1591283215: step 959, loss 0.584826, acc 0.772727\
1591283216: step 960, loss 0.723531, acc 0.545455\
1591283216: step 961, loss 0.67699, acc 0.727273\
1591283217: step 962, loss 0.616949, acc 0.727273\
1591283218: step 963, loss 0.338694, acc 0.954545\
1591283219: step 964, loss 0.799628, acc 0.772727\
1591283220: step 965, loss 0.789851, acc 0.681818\
1591283221: step 966, loss 1.39553, acc 0.5\
1591283221: step 967, loss 1.00621, acc 0.545455\
1591283222: step 968, loss 0.39303, acc 0.818182\
1591283223: step 969, loss 0.774783, acc 0.727273\
1591283224: step 970, loss 0.863872, acc 0.590909\
1591283225: step 971, loss 0.999189, acc 0.590909\
1591283225: step 972, loss 0.487493, acc 0.818182\
1591283226: step 973, loss 0.782682, acc 0.636364\
1591283227: step 974, loss 0.667023, acc 0.681818\
1591283228: step 975, loss 0.614911, acc 0.818182\
1591283229: step 976, loss 0.702391, acc 0.681818\
1591283230: step 977, loss 0.607217, acc 0.863636\
1591283230: step 978, loss 0.980104, acc 0.681818\
1591283231: step 979, loss 0.617023, acc 0.727273\
1591283232: step 980, loss 1.02057, acc 0.545455\
1591283233: step 981, loss 0.49029, acc 0.772727\
1591283234: step 982, loss 0.638599, acc 0.727273\
1591283235: step 983, loss 0.421181, acc 0.772727\
1591283235: step 984, loss 0.547779, acc 0.772727\
1591283236: step 985, loss 0.560859, acc 0.818182\
1591283237: step 986, loss 1.0271, acc 0.545455\
1591283238: step 987, loss 0.475891, acc 0.818182\
1591283239: step 988, loss 0.798921, acc 0.590909\
1591283240: step 989, loss 0.465993, acc 0.863636\
1591283240: step 990, loss 0.521362, acc 0.772727\
1591283241: step 991, loss 0.94697, acc 0.727273\
1591283242: step 992, loss 0.891788, acc 0.636364\
1591283243: step 993, loss 0.712467, acc 0.727273\
1591283244: step 994, loss 0.832403, acc 0.681818\
1591283245: step 995, loss 0.811368, acc 0.727273\
1591283245: step 996, loss 0.703218, acc 0.681818\
1591283246: step 997, loss 1.29066, acc 0.545455\
1591283247: step 998, loss 0.968636, acc 0.590909\
1591283248: step 999, loss 0.831182, acc 0.681818\
1591283249: step 1000, loss 0.426539, acc 0.863636\
++++++++++++++++++dev++++++++++++++1591283255: step 1000, loss 0.843218, acc 0.682093\
}